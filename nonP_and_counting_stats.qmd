---
title: "Non Parametric and Counting Statistics"
author: "Kt Nowak"
format: html
editor: visual
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}

library(rmarkdown)

library(tidyverse)

library(lubridate)

library(dplyr)

turbidity <- read_csv("ntu_data.csv")

grad_school <- read_csv("grad_school.csv")

dogwood <- read_csv("DogwoodSeeds.csv")


```
For each of the following questions, please provide your analysis and an interpretation (e.g., written as you would in a scientific publication).  If it helps to describe your result, add tables or figures to help make your case. For every case, explain why you chose the particular analysis you did and demonstrate the reasons from the data.

## 1.The FDA has suggested a change in a medication that has been shown to have detrimental side effects in half of the patients.  A clinical trial was conducted with nineteen patients; only three reported side effects.  Did the change make a significant difference?  
```{r}
successes <- 3
trials <- 19

null_prob <- 0.5

binom.test(successes, trials, p = null_prob, alternative = "less")


```
### we fail to reject the null hypothesis, there's not enough evidence to conclude that the new medication is significantly better



## 2.Two different environmental remediation treatments are evaluated for the impacts on turbidity (measured using the Nephelometric Turbidity Unit or NTU).  For regulatory reasons, turbidity is binned into four groups based on NTU: 0-15 ntu, 16-22 ntu, 23-30 ntu, and 31+ ntu.  Do both treatments have the same impact on turbidity?  Explain.

```{r}

turbidity <- turbidity %>%
  mutate(NTU_bin = case_when(
    NTU <= 15 ~ "0-15",
    NTU >= 16 & NTU <= 22 ~ "16-22",
    NTU >= 23 & NTU <= 30 ~ "23-30",
    NTU >= 31 ~ "31+"
  ))


observed_table <- table(turbidity$Treatment, turbidity$NTU_bin)

print(observed_table)

```

```{r}

row_totals <- rowSums(observed_table)

col_totals <- colSums(observed_table)

total_count <- sum(observed_table)

expected_table <- outer(row_totals, col_totals) / total_count

print(expected_table)


```
```{r}
T <- sum( (observed_table - expected_table)^2 / expected_table  )

print(T)

alpha <- 0.05

p <- 1.0 - alpha 

critical_value <- qchisq(p, df=1 )

print(critical_value)

p.value <- 1.0 - pchisq( T, df=1 )

print(p.value)

```
### The p value is significantly less than the critical value so we can reject the null hypothesis. We can conclude that there is an association between the use of treatment A or B and the turbidity bin the results fall into.


## 3.A dozen graduate students tried to determine if there was a relationship between their undergraduate GPA and their scores on the Graduate Records Examination.  Look at these data and determine the extent to which they are related.  Explain.
```{r}

ggplot(grad_school, aes(x = GPA, y = GRE)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Undergraduate GPA", y = "GRE Score", 
       title = "Relationship Between GPA and GRE Score")

```
```{r}

model <- lm(GRE ~ GPA, data = grad_school)
summary(model)

```
```{r}

loess_model <- loess(GRE ~ GPA, data = grad_school)

summary(loess_model)

```

```{r}

ggplot(grad_school, aes(x = GPA, y = GRE)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "blue") +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(x = "Undergraduate GPA", y = "GRE Score",
       title = "Relationship Between GPA and GRE Score")

fit <- loess( GRE ~ GPA, data=grad_school)

print(fit)

span_values <- seq(0.2, 1.0, by = 0.1)

ggplot(grad_school, aes(x = GPA, y = GRE)) +
  geom_point() +
  lapply(seq_along(span_values), function(i) {
    geom_smooth(method = "loess", span = span_values[i], se = FALSE, color = i)
  }) +
  scale_color_manual(values = c("red", "blue", "green", "purple", "orange")) +
  labs(x = "Undergraduate GPA", y = "GRE Score",
       title = "LOESS Curves with Different Spans")

```
### The relationship between GPA and GRE score can be resonable predicted by a non parametric local regression. A local regression can explain all 12 data points with a residual error of 32, meaning the predicted GRE scores deviate from the actual scores by about 32 points.

## 4.You are looking at fruit yield on dogwood.  You designed an experiment with four different treatments and measured the total yield in germinated seeds.  Are there differences in yield?  Explain.
```{r}
```


